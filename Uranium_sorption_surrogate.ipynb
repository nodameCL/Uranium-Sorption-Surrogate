{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9eeb11db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "print(pd.__version__)\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl \n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a778d7d4-2e97-4d49-af99-a4683866b96f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The scikit-learn version is 0.24.2.\n"
     ]
    }
   ],
   "source": [
    "import sklearn \n",
    "print('The scikit-learn version is {}.'.format(sklearn.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae53410c-fa2c-427f-8f47-57dfd3c1d410",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.11\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0161a2cf",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5409d84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Columns = [r'$c_1$', r'$c_2$',r'$N_s$',r'$A_s$',r'$U^{4+}$',r'$Na^+$','pH','pe', r'$logK_1$',\n",
    "           r'$logK_2$',r'$logK_c$',r'$logK_a$', r'$logK_{UO_2^{2+}}$',r'$logK_{U^{4+}}$', 'pH2', \n",
    "           r'$\\sigma_0$',r'$\\sigma_{\\beta}$',r'$\\sigma_d$',r'$\\psi_0$',r'$\\psi_{\\beta}$',r'$\\psi_d$','A2', r'$\\equiv SO^-$',\n",
    "           r'$\\equiv SOH_2^+$', r'$\\equiv SOH_2^+:Cl^{-}$', r'$\\equiv SO^-:UO_2^{2+}$',r'$\\equiv SO^-:Na^+$',r'$\\equiv SOH^0$',r'$\\equiv SO^-:U^{4+}$']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bd3e53a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset/DATABASE_AGAIN.txt', delimiter=r\"\\s+\", low_memory=False, names = Columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "deb60184-7302-4560-8b06-9da3ba3007aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12.0, 2.0, (1589411, 29))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.pH.max(), df.pH.min(), df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d92760e0-2d70-4e40-b165-15be1162a095",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load x_train, x_val, x_test \n",
    "inputs = [r'$c_1$', r'$N_s$',r'$A_s$',r'$U^{4+}$',r'$Na^+$','pH', r'$logK_1$',\n",
    "           r'$logK_2$',r'$logK_c$',r'$logK_a$', r'$logK_{UO_2^{2+}}$',r'$logK_{U^{4+}}$']\n",
    "\n",
    "targets = [r'$\\sigma_0$',r'$\\sigma_{\\beta}$',r'$\\sigma_d$',r'$\\psi_0$',r'$\\psi_{\\beta}$',r'$\\psi_d$', r'$\\equiv SO^-$',\n",
    "           r'$\\equiv SOH_2^+$', r'$\\equiv SO^-:UO_2^{2+}$',r'$\\equiv SO^-:Na^+$',r'$\\equiv SOH^0$',r'$\\equiv SO^-:U^{4+}$']\n",
    "\n",
    "df_train_norm = pd.read_csv('dataset/train_norm.csv')\n",
    "df_val_norm = pd.read_csv('dataset/val_norm.csv')\n",
    "df_test_norm = pd.read_csv('dataset/test_norm.csv')\n",
    "\n",
    "X_train_scaled = df_train_norm[inputs]\n",
    "y_train_scaled = df_train_norm[targets]\n",
    "\n",
    "X_val_scaled = df_val_norm[inputs]\n",
    "y_val_scaled = df_val_norm[targets]\n",
    "\n",
    "X_test_scaled = df_test_norm[inputs]\n",
    "y_test_scaled = df_test_norm[targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7157c517-e009-4c0c-a099-e5e6e4443b30",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1271528, 24), (158941, 24), (158942, 24), (1589411, 29))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape, df_val.shape, df_test.shape, df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd91e96b-d655-4405-aafc-d6b3bda2f709",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Train model using multiouput regressor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9fec3fdc-9794-4cc3-af5c-963a09fa2f88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.multioutput import MultiOutputRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ef78eae2-b285-42fd-862c-541b1a4c0188",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time is:  5144.91960903001 s\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "start_time = time.perf_counter()\n",
    "estimator_RF = RandomForestRegressor(n_estimators=100,\n",
    "                                     oob_score=True, \n",
    "                                     random_state=20\n",
    "                                    )\n",
    "regr_RF = MultiOutputRegressor(estimator_RF).fit(X_train_scaled, y_train_scaled)\n",
    "end_time = time.perf_counter()\n",
    "print('training time is: ', end_time - start_time, 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5ddcf5b2-35e8-4cca-9c5b-ef7a6c47854f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF test time is:  86.7091995239025 s\n"
     ]
    }
   ],
   "source": [
    "# predict on test data \n",
    "import time\n",
    "start_time = time.perf_counter()\n",
    "y_pred_multioutRF = regr_RF.predict(X_test_scaled)\n",
    "end_time = time.perf_counter() \n",
    "print('RF test time is: ', end_time - start_time, 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ba7d70ed-a4f1-4022-9f67-87102387b3e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save predicted data \n",
    "np.savetxt('res/y_pred_multioutRF_njobs1.txt', y_pred_multioutRF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcef5adb-b4fc-43c8-8cdb-ed2f84e3ec59",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Training on DNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b193671-3b21-4dc6-8908-3ac8b2f58031",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from model.pytorchtools import EarlyStopping\n",
    "import model.net as models \n",
    "from model.dataset import SurfaceComplexationDataset\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ba7a0f1-b668-4376-8841-edef9a1c4e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_optimizer(network, optimizer, learning_rate):\n",
    "    if optimizer == \"sgd\":\n",
    "        optimizer = optim.SGD(network.parameters(),\n",
    "                              lr=learning_rate, momentum=0.9)\n",
    "    elif optimizer == \"adam\":\n",
    "        optimizer = optim.Adam(network.parameters(),\n",
    "                               lr=learning_rate)\n",
    "    \n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n",
    "    return optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc76021f-485f-420b-88f0-8c567174dfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_dir): \n",
    "    train_set = SurfaceComplexationDataset(root_dir=data_dir, split = 'train')\n",
    "    test_set = SurfaceComplexationDataset(root_dir=data_dir, split='test')\n",
    "    val_set = SurfaceComplexationDataset(root_dir=data_dir, split='val')\n",
    "\n",
    "    return train_set, val_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88983817-dea2-4304-97ce-0202bb6a35eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(train_loader, model, optimizer, device, epoch):\n",
    "    \"\"\" Train the model on num_steps batches \n",
    "    Args: \n",
    "        train_loader: a torch.utils.data.DataLoader object that fetches the data\n",
    "        model: the neural network \n",
    "        optimizer: adams \n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    num_batch = len(train_loader)\n",
    "\n",
    "    for i, (inputs, targets) in enumerate(train_loader): \n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        # zero the paramter gradients \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize \n",
    "        pred = model(inputs)\n",
    "        loss = F.mse_loss(pred, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics \n",
    "        running_loss += loss.item()\n",
    "        # if i % 300 == 0: \n",
    "        #     print('[%d: %d/%d] train loss: %f ' % (epoch, i, num_batch, loss.item()))\n",
    "        # if i % 300 == 0: \n",
    "        #     print('[%d: %d/%d] train loss: %f lr = %f' % (epoch, i, num_batch, loss.item(), optimizer.param_groups[0][\"lr\"]))\n",
    "\n",
    "    return running_loss / num_batch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89085dff-3225-4798-a4e9-dd0bd49d7418",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(val_dataloader, model, device): \n",
    "    model.eval()\n",
    "    val_running_loss = 0.0 \n",
    "\n",
    "    with torch.no_grad(): \n",
    "        for inputs, targets in val_dataloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = F.mse_loss(outputs, targets)\n",
    "\n",
    "            val_running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    return val_running_loss / len(val_dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0e6dd2e-0fa0-4f0e-a65f-c1b119ed8752",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_pramas(test_y, test_pred, foldername, outfile): \n",
    "    # print(\"R2 of training is: \", r2_score(train_y, train_pred))\n",
    "    np.savetxt(f'{foldername}/test_pred_{outfile}.txt', test_pred)\n",
    "    np.savetxt(f'{foldername}/test_y_{outfile}.txt', test_y)\n",
    "    \n",
    "    print(\"R2 of test is: \", r2_score(test_y, test_pred))\n",
    "\n",
    "    test_mse = mean_squared_error(test_y, test_pred)\n",
    "    test_mae = mean_absolute_error(test_y, test_pred)\n",
    "\n",
    "    print('Test set results for %i samples:' % test_pred.shape[0])\n",
    "    print('MSE:', test_mse)\n",
    "    print('MAE:', test_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8886ee5b-0b69-4388-aab2-0854cf4caa18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_accuracy(net, testloader, foldername, outfile, device): \n",
    "    test_pred = []\n",
    "    test_y = []\n",
    "    running_loss = 0 \n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            inputs, targets = data\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = F.mse_loss(outputs, targets)\n",
    "\n",
    "            running_loss += loss.item() \n",
    "\n",
    "            pred_val_numpy = outputs.data.cpu().numpy()\n",
    "            target_val_numpy = targets.data.cpu().numpy()\n",
    "\n",
    "            test_pred.append(pred_val_numpy)\n",
    "            test_y.append(target_val_numpy)\n",
    "\n",
    "    test_pred = np.concatenate(test_pred, axis=0)\n",
    "    test_y = np.concatenate(test_y, axis=0)\n",
    "\n",
    "    plot_pramas(test_y, test_pred, foldername, outfile)\n",
    "\n",
    "    print('MSE loss on test set is:', running_loss / len(testloader.dataset)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c448dc1e-f6c2-4458-a066-02b53909a7ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(model, device, train_loader, val_loader, test_loader, optimizer, lr_scheduler, isSch, res_dir, name, patience = 20, n_epochs = 100): \n",
    "    # to track the average training loss per epoch as the model trains\n",
    "    avg_train_losses = []\n",
    "    # to track the average validation loss per epoch as the model trains\n",
    "    avg_valid_losses = [] \n",
    "\n",
    "    blue = lambda x: '\\033[94m' + x + '\\033[0m'\n",
    "    \n",
    "    checkpoint_dir = os.path.join(res_dir, 'checkpoints')\n",
    "    try:\n",
    "        os.makedirs(res_dir)\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    except OSError:\n",
    "        pass\n",
    "\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, f'{name}.pt')\n",
    "    # initialize the early_stopping object\n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=True, path = checkpoint_path)\n",
    "\n",
    "    for epoch in tqdm(range(1, n_epochs + 1)):\n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        train_epoch_loss = train_epoch(train_loader, model, optimizer, device, epoch)\n",
    "        val_epoch_loss = validate(val_loader, model, device)\n",
    "\n",
    "        # print loss every epoch \n",
    "        print('[%d] train loss: %f ' % (epoch, train_epoch_loss))\n",
    "        print('[%d] %s loss: %f' % (epoch, blue('validate'), val_epoch_loss))\n",
    "\n",
    "        avg_train_losses.append(train_epoch_loss)\n",
    "        avg_valid_losses.append(val_epoch_loss)\n",
    "        \n",
    "        if isSch: \n",
    "            lr_scheduler.step(val_epoch_loss) \n",
    "        \n",
    "        # add early stopping \n",
    "        # early_stopping(val_epoch_loss, model)\n",
    "        early_stopping(train_epoch_loss, model)\n",
    "        if early_stopping.early_stop: \n",
    "            print(\"Early stopping\")\n",
    "            break \n",
    "\n",
    "    np.savetxt(os.path.join(res_dir, f'train_loss_{name}.csv'), avg_train_losses)\n",
    "    np.savetxt(os.path.join(res_dir, f'val_loss_{name}.csv'), avg_valid_losses) \n",
    "\n",
    "    # load the last checkpoint with the best model\n",
    "    model.load_state_dict(torch.load(checkpoint_path)) \n",
    "\n",
    "    # test on test set \n",
    "    test_accuracy(model, test_loader, res_dir, name, device)\n",
    "    # print(optimizer.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4b815f70-4f10-4d8e-ae50-5f76116cf48d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_main(config): \n",
    "    data_dir = 'dataset/'\n",
    "\n",
    "    # get dataset \n",
    "    train_set, val_set, test_set = load_data(data_dir)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_set,\n",
    "        batch_size=int(config[\"batch_size\"]),\n",
    "        shuffle=True,\n",
    "        num_workers=4, \n",
    "        pin_memory=False)\n",
    "\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "            val_set,\n",
    "            batch_size=int(config[\"batch_size\"]),\n",
    "            shuffle=True,\n",
    "            num_workers=4, \n",
    "            pin_memory=False)\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "            test_set, \n",
    "            batch_size=int(config[\"batch_size\"]), \n",
    "            shuffle=True,\n",
    "            num_workers=4, \n",
    "            pin_memory=False)\n",
    "            \n",
    "    print(\"Creating model\")\n",
    "    Model = getattr(models, config['model'])\n",
    "    print('created model is: ', Model)\n",
    "\n",
    "    model = Model(config['batch_norm'], config['layer_norm'], \n",
    "                 config[\"l1\"], config[\"l2\"], config[\"l3\"], config[\"l4\"], config[\"l5\"])\n",
    "\n",
    "    name = f\"{config['model']}_{config['l1']}{config['l2']}{config['l3']}{config['l4']}\" +\\\n",
    "            f\"{config['l5']}lr{config['lr']}BS{config['batch_size']}isB{config['batch_norm']}\" +\\\n",
    "            f\"ln{config['layer_norm']}Opt{config['optimizer']}sch{config['lr_scheduler']}\"\n",
    "        \n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer, lr_scheduler = build_optimizer(model, config['optimizer'], config['lr'])\n",
    "    res_dir = 'DNN_res/'\n",
    "    \n",
    "    train_model(model, device, train_loader, val_loader, test_loader, optimizer, lr_scheduler, config['lr_scheduler'], res_dir, name, 20, 5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae86b501-273b-4d5d-ad98-76acb54257e3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## train model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f90f836-7c68-4988-b249-55480ba7f657",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'l1': 512, 'l2': 512, 'l3': 512, 'l4': 512, 'l5': 512, \n",
    "          'lr': 0.001, 'batch_size': 128, 'model': 'DeepNet6LayerTune', 'batch_norm': False, \n",
    "          'layer_norm': True, 'lr_scheduler': True, 'optimizer': 'adam'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3db3372-db64-434b-b45b-eec6c9bbf5c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time \n",
    "start_time = time.perf_counter()\n",
    "train_main(config)\n",
    "end_time = time.perf_counter() \n",
    "print(\"time used to train model with 20/5000 patience is: \", (end_time - start_time)/60, 'mins', (end_time - start_time) / 3600, 'hrs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116f0821-2e46-42af-84d4-06059115ec65",
   "metadata": {
    "tags": []
   },
   "source": [
    "# test model performance with two additional test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d90da2-2f11-45f5-a21c-14f5655333be",
   "metadata": {
    "tags": []
   },
   "source": [
    "### RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "85a0b918-9e28-401d-b682-741a6ccf6443",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_random_scale = pd.read_csv('dataset/set1/test_set1.csv')\n",
    "# df_random_scale = pd.read_csv('dataset/set2/test_set2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "49c93484-e734-478b-aaec-ed501f2fc160",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# RF regressor \n",
    "import joblib\n",
    "regr_RF = joblib.load('newRF_multiOutRegressor_nest100_oobTrue_rand20_njobs1.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "866b927b-e8e0-4838-b39a-7f4f005ffd79",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF test time is:  0.09720314387232065 s\n"
     ]
    }
   ],
   "source": [
    "# predict on test data \n",
    "import time\n",
    "start_time = time.perf_counter()\n",
    "y_pred_set1_multioutRF = regr_RF.predict(df_random_scale[inputs])\n",
    "end_time = time.perf_counter() \n",
    "print('RF test time is: ', end_time - start_time, 's')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cbb4ec-52e3-40de-b02d-cae77e4b0b2b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cbaeff27-b020-4477-8553-658573b81504",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_on_test(config, data_dir = 'dataset/set1/', testdata = 'set1'): \n",
    "\n",
    "    # get dataset \n",
    "    _, _, test_set = load_data(data_dir)\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "            test_set, \n",
    "            batch_size=int(config[\"batch_size\"]), \n",
    "            shuffle=True,\n",
    "            num_workers=4, \n",
    "            pin_memory=False)\n",
    "            \n",
    "    print(\"Creating model\")\n",
    "    Model = getattr(models, config['model'])\n",
    "    print('created model is: ', Model)\n",
    "    \n",
    "    model = Model(config['batch_norm'], config['layer_norm'], \n",
    "                     config[\"l1\"], config[\"l2\"], config[\"l3\"], config[\"l4\"], config[\"l5\"])\n",
    "        \n",
    "    name = f\"{config['model']}_{config['l1']}{config['l2']}{config['l3']}{config['l4']}{config['l5']}lr{config['lr']}BS{config['batch_size']}isB{config['batch_norm']}ln{config['layer_norm']}Opt{config['optimizer']}sch{config['lr_scheduler']}\"\n",
    "    \n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer, lr_scheduler = build_optimizer(model, config['optimizer'], config['lr'])\n",
    "    \n",
    "    res_dir = 'DNN_res'\n",
    "    checkpoint_dir = os.path.join(res_dir, 'checkpoints')\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, f'{name}.pt')\n",
    "    \n",
    "    # load the last checkpoint with the best model\n",
    "    model.load_state_dict(torch.load(checkpoint_path)) \n",
    "\n",
    "    # test on test set \n",
    "    test_accuracy(model, test_loader, res_dir, testdata+name, device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "933f39ef-6c4b-40e1-aca8-3b415877f80d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model\n",
      "created model is:  <class 'model.net.DeepNet6LayerTune'>\n",
      "R2 of test is:  0.9999466318557761\n",
      "Test set results for 158942 samples:\n",
      "MSE: 8.477168e-07\n",
      "MAE: 0.00057548966\n",
      "MSE loss on test set is: 6.624687552514079e-09\n",
      "time used to evaluate the model on test data:  13.97365738498047 sec 0.2328942897496745 mins\n"
     ]
    }
   ],
   "source": [
    "config = {'l1': 512, 'l2': 512, 'l3': 512, 'l4': 512, 'l5': 512, \n",
    "          'lr': 0.001, 'batch_size': 128, 'model': 'DeepNet6LayerTune', 'batch_norm': False, \n",
    "          'layer_norm': True, 'lr_scheduler': True, 'optimizer': 'adam'}\n",
    "\n",
    "import time \n",
    "start_time = time.perf_counter()\n",
    "predict_on_test(config, data_dir='dataset/', testdata='test')\n",
    "end_time = time.perf_counter()\n",
    "print('time used to evaluate the model on test data: ', end_time - start_time, 'sec', (end_time - start_time) / 60, 'mins')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17f4cf4c-5f05-471b-b175-e642e9a3f866",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = {'l1': 512, 'l2': 512, 'l3': 512, 'l4': 512, 'l5': 512, \n",
    "          'lr': 0.001, 'batch_size': 128, 'model': 'DeepNet6LayerTune', 'batch_norm': False, \n",
    "          'layer_norm': True, 'lr_scheduler': True, 'optimizer': 'adam'}\n",
    "\n",
    "import time \n",
    "predict_on_test(config, data_dir='dataset/set2/', testdata='set2')\n",
    "# predict_on_test(config, data_dir='dataset/set1/', testdata='set1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b050e56a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NERSC Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
